{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "وارد کردن کتابخانه‌ها\n",
    "</font>\n",
    "</h2>\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در این بخش، کتابخانه‌های مختلفی برای انجام پردازش‌های مختلف بر روی داده‌ها وارد می‌شوند. این کتابخانه‌ها شامل <b>random</b> برای تولید اعداد تصادفی، <b>re</b> برای کار با عبارات منظم، <b>nltk</b> برای پردازش زبان طبیعی، <b>pandas</b> برای تجزیه و تحلیل داده‌ها، <b>math</b> برای محاسبات ریاضی، و <b>matplotlib</b> برای رسم نمودارها هستند. همچنین برای توکنایز کردن داده‌ها، کتابخانه <b>nltk</b> بارگذاری می‌شود.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "خواندن داده‌ها\n",
    "</font>\n",
    "</h2>\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در این بخش داده‌ها از فایل متنی <b>twits.txt</b> بارگذاری می‌شوند. پس از خواندن داده‌ها، نوع داده و طول آن و همچنین اولین ۵۰۰ کاراکتر آن چاپ می‌شود تا بررسی اولیه‌ای از داده‌ها داشته باشیم.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'str'>\n",
      "Number of characters: 3256325\n",
      "First 500 characters: How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\n",
      "When you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\n",
      "they've decided its more fun if I don't.\n",
      "So Tired D; Played Lazer Tag & Ran A LOT D; Ughh Going To Sleep Like In 5 Minutes ;)\n",
      "Words from a complete stranger! Made my birthday even better :)\n",
      "First Cubs game ever! Wrigley field is gorgeous. This is perfect. Go Cubs Go!\n",
      "i no! i ge\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "with open(\"../Data/twits.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(\"Data type:\", type(data))\n",
    "print(\"Number of characters:\", len(data))\n",
    "print(\"First 500 characters:\", data[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "توکنایز کردن متن\n",
    "</font>\n",
    "</h2>\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در این بخش، باید تابعی برای پیش‌پردازش داده‌ها بنویسید که شامل مراحل زیر باشد:\n",
    "<ul>\n",
    "    <li>تبدیل تمام متن به حروف کوچک</li>\n",
    "    <li>حذف علائم نگارشی</li>\n",
    "    <li>توکنایز کردن متن به جملات و سپس به کلمات</li>\n",
    "</ul>\n",
    "شما باید این توابع را به گونه‌ای بنویسید که بتوانید داده‌ها را آماده استفاده در مراحل بعدی کنید.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed Sentences:\n",
      "['how', 'are', 'you', 'btw', 'thanks']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the input text: lowercase, remove special characters, and tokenize sentences.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', '', text)\n",
    "    sentences = word_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "# Preprocess the data\n",
    "sentences = preprocess_text(data)\n",
    "\n",
    "# Display the first 5 sentences\n",
    "print(\"\\nPreprocessed Sentences:\")\n",
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "جایگزینی کلمات خارج از واژه‌نامه با <b>&lt;unk&gt;</b>\n",
    "</font>\n",
    "</h2>\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در این مرحله شما باید تابعی بنویسید که کلمات خارج از واژه‌نامه را با کلمه خاصی مانند <b>&lt;unk&gt;</b> جایگزین کند. این تابع باید به طور خودکار کلماتی که در واژه‌نامه وجود ندارند را شناسایی کرده و آن‌ها را با این کلمه خاص جایگزین نماید.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "    \"\"\"\n",
    "    Replace words not in the given vocabulary with '<unk>' token.\n",
    "    \"\"\"\n",
    "    a = []\n",
    "    for words in tokenized_sentences:\n",
    "        if words in vocabulary:\n",
    "            a.append(words)\n",
    "        else:\n",
    "            a.append(unknown_token)\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "استخراج کلمات با فرکانس n یا بیشتر\n",
    "</font>\n",
    "</h2>\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در این بخش باید تابعی بنویسید که کلمات با فرکانس n یا بیشتر را از داده‌ها استخراج کند. این کار کمک می‌کند تا فقط کلمات پرکاربرد در مدل استفاده شوند و کلمات نادر که ممکن است در پیش‌بینی‌های مدل اثر منفی داشته باشند، حذف شوند.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'to',\n",
       " 'i',\n",
       " 'a',\n",
       " 'you',\n",
       " 'and',\n",
       " 'for',\n",
       " 'in',\n",
       " 'is',\n",
       " 'of',\n",
       " 'it',\n",
       " 'my',\n",
       " 'on',\n",
       " 'that',\n",
       " 'me',\n",
       " 'be',\n",
       " 'at',\n",
       " 'with',\n",
       " 'so',\n",
       " 'your',\n",
       " 'have',\n",
       " 'this',\n",
       " 'are',\n",
       " 'im',\n",
       " 'just',\n",
       " 'we',\n",
       " 'not',\n",
       " 'like',\n",
       " 'but',\n",
       " 'its',\n",
       " 'all',\n",
       " 'was',\n",
       " 'out',\n",
       " 'up',\n",
       " 'get',\n",
       " 'what',\n",
       " 'if',\n",
       " 'love',\n",
       " 'do',\n",
       " 'good',\n",
       " 'can',\n",
       " 'will',\n",
       " 'thanks',\n",
       " 'about',\n",
       " 'day',\n",
       " 'dont',\n",
       " 'rt',\n",
       " 'now',\n",
       " 'one',\n",
       " 'from',\n",
       " 'when',\n",
       " 'know',\n",
       " 'no',\n",
       " 'great',\n",
       " 'u',\n",
       " 'time',\n",
       " 'go',\n",
       " 'how',\n",
       " 'today',\n",
       " 'or',\n",
       " 'an',\n",
       " 'they',\n",
       " 'new',\n",
       " 'as',\n",
       " 'lol',\n",
       " 'see',\n",
       " 'got',\n",
       " 'more',\n",
       " 'our',\n",
       " 'by',\n",
       " 'there',\n",
       " 'some',\n",
       " 'back',\n",
       " 'he',\n",
       " 'too',\n",
       " 'who',\n",
       " 'going',\n",
       " 'cant',\n",
       " 'us',\n",
       " 'would',\n",
       " 'need',\n",
       " 'think',\n",
       " 'follow',\n",
       " 'people',\n",
       " 'want',\n",
       " 'happy',\n",
       " 'well',\n",
       " 'right',\n",
       " 'has',\n",
       " 'tonight',\n",
       " 'make',\n",
       " 'been',\n",
       " 'really',\n",
       " 'only',\n",
       " 'work',\n",
       " 'were',\n",
       " 'am',\n",
       " '3',\n",
       " 'thats',\n",
       " 'much',\n",
       " 'thank',\n",
       " 'should',\n",
       " 'had',\n",
       " 'night',\n",
       " 'come',\n",
       " 'did',\n",
       " 'youre',\n",
       " 'why',\n",
       " 'here',\n",
       " 'off',\n",
       " 'still',\n",
       " 'them',\n",
       " 'na',\n",
       " 'best',\n",
       " 'her',\n",
       " 'last',\n",
       " 'way',\n",
       " 'his',\n",
       " 'never',\n",
       " '2',\n",
       " 'hope',\n",
       " 'ill',\n",
       " 'life',\n",
       " 'say',\n",
       " 'then',\n",
       " 'oh',\n",
       " 'show',\n",
       " 'over',\n",
       " 'first',\n",
       " 'twitter',\n",
       " 'next',\n",
       " 'than',\n",
       " 'she',\n",
       " 'game',\n",
       " 'week',\n",
       " 'their',\n",
       " 'better',\n",
       " 'please',\n",
       " 'always',\n",
       " '“',\n",
       " 'tomorrow',\n",
       " 'any',\n",
       " 'down',\n",
       " 'yes',\n",
       " 'wait',\n",
       " 'looking',\n",
       " '”',\n",
       " 'year',\n",
       " 'take',\n",
       " 'feel',\n",
       " 'where',\n",
       " 'haha',\n",
       " 'him',\n",
       " 'look',\n",
       " 'ever',\n",
       " 'man',\n",
       " 'could',\n",
       " 'awesome',\n",
       " 'everyone',\n",
       " 'very',\n",
       " 'getting',\n",
       " 'home',\n",
       " 'even',\n",
       " 'let',\n",
       " 'keep',\n",
       " 'hey',\n",
       " 'big',\n",
       " 'morning',\n",
       " 'ive',\n",
       " 'yeah',\n",
       " 'because',\n",
       " 'something',\n",
       " 'sure',\n",
       " 'after',\n",
       " 'again',\n",
       " 'guys',\n",
       " 'thing',\n",
       " 'into',\n",
       " 'world',\n",
       " 'live',\n",
       " 'bad',\n",
       " 'fun',\n",
       " 'tweet',\n",
       " 'doing',\n",
       " 'd',\n",
       " 'gon',\n",
       " 'check',\n",
       " 'nice',\n",
       " 'someone',\n",
       " 'those',\n",
       " 'weekend',\n",
       " 'being',\n",
       " 'watching',\n",
       " 'help',\n",
       " 'tell',\n",
       " 'lets',\n",
       " 'didnt',\n",
       " 'school',\n",
       " 'free',\n",
       " 'many',\n",
       " 'little',\n",
       " 'things',\n",
       " 'soon',\n",
       " 'amazing',\n",
       " 'music',\n",
       " 'other',\n",
       " 'wish',\n",
       " 'miss',\n",
       " 'give',\n",
       " 'every',\n",
       " 'w',\n",
       " 'anyone',\n",
       " '4',\n",
       " 'days',\n",
       " 'said',\n",
       " 'hate',\n",
       " 'before',\n",
       " 'long',\n",
       " 'whats',\n",
       " 'another',\n",
       " 'start',\n",
       " 'made',\n",
       " 'most',\n",
       " 'stop',\n",
       " 'may',\n",
       " 'same',\n",
       " 'call',\n",
       " 'also',\n",
       " 'coming',\n",
       " 'watch',\n",
       " 'friends',\n",
       " 'shit',\n",
       " 'done',\n",
       " 'birthday',\n",
       " 'girl',\n",
       " 'does',\n",
       " '1',\n",
       " 'find',\n",
       " 'win',\n",
       " 'cool',\n",
       " 'wan',\n",
       " 'use',\n",
       " 'pretty',\n",
       " 'ready',\n",
       " 'old',\n",
       " 'talk',\n",
       " 'real',\n",
       " 'god',\n",
       " 'having',\n",
       " 'working',\n",
       " 'though',\n",
       " 'play',\n",
       " 'nothing',\n",
       " 'ur',\n",
       " 'makes',\n",
       " 'thought',\n",
       " 'these',\n",
       " 'excited',\n",
       " 'doesnt',\n",
       " 's',\n",
       " 'team',\n",
       " 'making',\n",
       " 'glad',\n",
       " 'two',\n",
       " 'around',\n",
       " 'sorry',\n",
       " 'years',\n",
       " 'welcome',\n",
       " 'ok',\n",
       " 'hard',\n",
       " 'try',\n",
       " 'house',\n",
       " 'maybe',\n",
       " 'wow',\n",
       " 'hear',\n",
       " 'saw',\n",
       " 'party',\n",
       " 'already',\n",
       " 'away',\n",
       " 'forward',\n",
       " 'hes',\n",
       " 'favorite',\n",
       " 'put',\n",
       " 'while',\n",
       " 'friday',\n",
       " 'ass',\n",
       " 'yet',\n",
       " 'job',\n",
       " 'lot',\n",
       " 'through',\n",
       " 'following',\n",
       " 'friend',\n",
       " 'beautiful',\n",
       " 'looks',\n",
       " '5',\n",
       " 'stay',\n",
       " 'might',\n",
       " 'trying',\n",
       " 'until',\n",
       " 'damn',\n",
       " 'ya',\n",
       " 'guy',\n",
       " 'end',\n",
       " 'everything',\n",
       " 'song',\n",
       " 'actually',\n",
       " '’',\n",
       " 'mean',\n",
       " 'which',\n",
       " 'sleep',\n",
       " 'food',\n",
       " 'person',\n",
       " 'guess',\n",
       " 'baby',\n",
       " 'isnt',\n",
       " 'own',\n",
       " 'family',\n",
       " 'true',\n",
       " 'book',\n",
       " 'phone',\n",
       " 'mom',\n",
       " 'name',\n",
       " 'run',\n",
       " 'fuck',\n",
       " 'crazy',\n",
       " 'season',\n",
       " 'change',\n",
       " 'id',\n",
       " 'support',\n",
       " 'open',\n",
       " 'since',\n",
       " 'anything',\n",
       " 'must',\n",
       " 'read',\n",
       " 'congrats',\n",
       " 'wont',\n",
       " 'class',\n",
       " 'video',\n",
       " 'theres',\n",
       " 'hell',\n",
       " 'place',\n",
       " 'enjoy',\n",
       " 'few',\n",
       " 'sounds',\n",
       " 'sweet',\n",
       " 'far',\n",
       " 'high',\n",
       " 'kids',\n",
       " 'stuff',\n",
       " '10',\n",
       " 'ta',\n",
       " 'eat',\n",
       " 'early',\n",
       " 'meet',\n",
       " 'else',\n",
       " 'such',\n",
       " 'playing',\n",
       " 'head',\n",
       " 'remember',\n",
       " 'finally',\n",
       " 'talking',\n",
       " 'omg',\n",
       " 'news',\n",
       " 'movie',\n",
       " 'hit',\n",
       " 'city',\n",
       " 'thinking',\n",
       " 'ha',\n",
       " 'says',\n",
       " 'cause',\n",
       " 'summer',\n",
       " 'followers',\n",
       " 'left',\n",
       " 'money',\n",
       " 'hi',\n",
       " 'email',\n",
       " 'facebook',\n",
       " 'went',\n",
       " 'send',\n",
       " 'buy',\n",
       " 'without',\n",
       " 'times',\n",
       " 'ask',\n",
       " 'idea',\n",
       " 'funny',\n",
       " 'heart',\n",
       " 'care',\n",
       " 'sometimes',\n",
       " 'believe',\n",
       " 'n',\n",
       " 'myself',\n",
       " 'part',\n",
       " 'havent',\n",
       " 'r',\n",
       " 'enough',\n",
       " 'business',\n",
       " 'both',\n",
       " 'hours',\n",
       " 'hot',\n",
       " 'room',\n",
       " 'luck',\n",
       " 'event',\n",
       " 'needs',\n",
       " 'tweets',\n",
       " 'post',\n",
       " 'seen',\n",
       " 'probably',\n",
       " 'late',\n",
       " 'sunday',\n",
       " 'bed',\n",
       " 'meeting',\n",
       " 'bring',\n",
       " 'perfect',\n",
       " 'kind',\n",
       " 'taking',\n",
       " 'p',\n",
       " 'feeling',\n",
       " 'bro',\n",
       " 'mind',\n",
       " 'black',\n",
       " 'girls',\n",
       " 'heard',\n",
       " 'monday',\n",
       " 'seeing',\n",
       " 'word',\n",
       " 'words',\n",
       " 'together',\n",
       " 'hour',\n",
       " 'social',\n",
       " 'wrong',\n",
       " 'story',\n",
       " 'join',\n",
       " 'car',\n",
       " 'whole',\n",
       " 'called',\n",
       " 'once',\n",
       " 'break',\n",
       " 'almost',\n",
       " 'saturday',\n",
       " 'yourself',\n",
       " 'lmao',\n",
       " 'fucking',\n",
       " 'listening',\n",
       " 'face',\n",
       " 'forget',\n",
       " 'hair',\n",
       " 'special',\n",
       " 'least',\n",
       " 'used',\n",
       " 'point',\n",
       " 'saying',\n",
       " 'super',\n",
       " 'dinner',\n",
       " 'la',\n",
       " 'reading',\n",
       " 'lost',\n",
       " 'later',\n",
       " 'aint',\n",
       " 'top',\n",
       " 'tired',\n",
       " 'bitch',\n",
       " 'youll',\n",
       " 'coffee',\n",
       " 'sad',\n",
       " 'gets',\n",
       " 'dude',\n",
       " 'started',\n",
       " 'theyre',\n",
       " 'o',\n",
       " 'games',\n",
       " 'okay',\n",
       " 'found',\n",
       " 'set',\n",
       " 'tv',\n",
       " 'definitely',\n",
       " 'lunch',\n",
       " 'full',\n",
       " 'fans',\n",
       " 'yall',\n",
       " 'bit',\n",
       " 'question',\n",
       " 'loved',\n",
       " '1st',\n",
       " 'hahaha',\n",
       " 'mothers',\n",
       " 'red',\n",
       " 'media',\n",
       " 'less',\n",
       " 'totally',\n",
       " 'different',\n",
       " 'yo',\n",
       " 'b',\n",
       " 'missed',\n",
       " 'line',\n",
       " 'boy',\n",
       " 'shes',\n",
       " 'women',\n",
       " 'bout',\n",
       " 'text',\n",
       " 'wanted',\n",
       " 'chance',\n",
       " '20',\n",
       " 'goes',\n",
       " 'men',\n",
       " 'austin',\n",
       " 'agree',\n",
       " 'online',\n",
       " 'band',\n",
       " 'happen',\n",
       " 'came',\n",
       " 'learn',\n",
       " 'told',\n",
       " 'comes',\n",
       " 'during',\n",
       " 'month',\n",
       " 'sick',\n",
       " 'seriously',\n",
       " 'rock',\n",
       " 'wants',\n",
       " 'yesterday',\n",
       " '2012',\n",
       " 'office',\n",
       " '7',\n",
       " 'pick',\n",
       " '6',\n",
       " 'state',\n",
       " 'starting',\n",
       " 'waiting',\n",
       " 'means',\n",
       " 'weather',\n",
       " 'minutes',\n",
       " 'listen',\n",
       " 'close',\n",
       " 'weeks',\n",
       " 'move',\n",
       " 'interesting',\n",
       " 'wonder',\n",
       " 'using',\n",
       " 'course',\n",
       " 'ago',\n",
       " 'cute',\n",
       " 'understand',\n",
       " 'future',\n",
       " 'students',\n",
       " 'problem',\n",
       " 'share',\n",
       " 'hello',\n",
       " 'giving',\n",
       " 'info',\n",
       " 'seems',\n",
       " 'each',\n",
       " 'drink',\n",
       " 'proud',\n",
       " 'stupid',\n",
       " 'c',\n",
       " '12',\n",
       " 'eyes',\n",
       " 'turn',\n",
       " 'leave',\n",
       " 'mad',\n",
       " 'art',\n",
       " 'power',\n",
       " 'fan',\n",
       " 'yea',\n",
       " 'lots',\n",
       " 'vs',\n",
       " 'page',\n",
       " 'dog',\n",
       " 'took',\n",
       " 'second',\n",
       " 'visit',\n",
       " 'half',\n",
       " 'site',\n",
       " 'white',\n",
       " 'appreciate',\n",
       " 'chicago',\n",
       " 'drive',\n",
       " 'outside',\n",
       " 'dear',\n",
       " 'boys',\n",
       " 'beer',\n",
       " 'others',\n",
       " 'cold',\n",
       " 'beat',\n",
       " 'able',\n",
       " 'dream',\n",
       " 'list',\n",
       " 'pm',\n",
       " 'walk',\n",
       " 'moment',\n",
       " 'three',\n",
       " 'via',\n",
       " 'bar',\n",
       " 'sent',\n",
       " 'whos',\n",
       " 'running',\n",
       " 'side',\n",
       " 'gone',\n",
       " 'cuz',\n",
       " 'dm',\n",
       " 'mine',\n",
       " 'ugh',\n",
       " 'success',\n",
       " '8',\n",
       " 'dance',\n",
       " 'rest',\n",
       " 'app',\n",
       " 'website',\n",
       " 'account',\n",
       " 'thursday',\n",
       " 'either',\n",
       " 'youve',\n",
       " 't',\n",
       " 'ones',\n",
       " 'park',\n",
       " 'trip',\n",
       " 'fall',\n",
       " 'wouldnt',\n",
       " 'worth',\n",
       " 'between',\n",
       " 'reason',\n",
       " 'past',\n",
       " 'ppl',\n",
       " 'shout',\n",
       " 'wonderful',\n",
       " 'ff',\n",
       " 'st',\n",
       " 'number',\n",
       " 'san',\n",
       " 'ah',\n",
       " 'huge',\n",
       " 'starts',\n",
       " 'young',\n",
       " 'nyc',\n",
       " 'fine',\n",
       " 'instead',\n",
       " 'google',\n",
       " 'wasnt',\n",
       " 'date',\n",
       " 'history',\n",
       " 'single',\n",
       " 'radio',\n",
       " 'film',\n",
       " 'rain',\n",
       " 'writing',\n",
       " 'kid',\n",
       " 'works',\n",
       " 'christmas',\n",
       " 'spring',\n",
       " 'busy',\n",
       " 'till',\n",
       " 'gave',\n",
       " 'x',\n",
       " 'college',\n",
       " 'water',\n",
       " 'dead',\n",
       " 'living',\n",
       " 'finished',\n",
       " 'service',\n",
       " 'sound',\n",
       " 'plan',\n",
       " 'followed',\n",
       " 'forever',\n",
       " 'deal',\n",
       " 'bc',\n",
       " 'yay',\n",
       " 'wearing',\n",
       " 'john',\n",
       " '15',\n",
       " 'fact',\n",
       " 'save',\n",
       " '30',\n",
       " '100',\n",
       " 'interested',\n",
       " 'matter',\n",
       " 'add',\n",
       " 'tried',\n",
       " 'todays',\n",
       " 'tweeting',\n",
       " 'asked',\n",
       " 'won',\n",
       " 'conference',\n",
       " 'heat',\n",
       " 'knows',\n",
       " 'fire',\n",
       " 'blog',\n",
       " 'hopefully',\n",
       " 'weird',\n",
       " 'fast',\n",
       " 'american',\n",
       " 'front',\n",
       " 'em',\n",
       " 'books',\n",
       " 'become',\n",
       " 'club',\n",
       " 'knew',\n",
       " 'picture',\n",
       " 'tour',\n",
       " 'da',\n",
       " 'pay',\n",
       " 'loving',\n",
       " 'tickets',\n",
       " 'justin',\n",
       " 'lady',\n",
       " 'order',\n",
       " 'woman',\n",
       " 'cut',\n",
       " 'tho',\n",
       " 'couldnt',\n",
       " 'project',\n",
       " 'country',\n",
       " 'street',\n",
       " 'experience',\n",
       " 'series',\n",
       " 'dad',\n",
       " 'wine',\n",
       " 'behind',\n",
       " 'write',\n",
       " 'iphone',\n",
       " 'light',\n",
       " 'group',\n",
       " 'final',\n",
       " 'learning',\n",
       " 'everybody',\n",
       " 'safe',\n",
       " 'thx',\n",
       " 'bored',\n",
       " 'local',\n",
       " 'answer',\n",
       " '9',\n",
       " 'small',\n",
       " 'smile',\n",
       " 'football',\n",
       " 'shot',\n",
       " 'lose',\n",
       " 'session',\n",
       " 'hand',\n",
       " 'brother',\n",
       " 'worst',\n",
       " 'gift',\n",
       " 'son',\n",
       " 'wake',\n",
       " 'album',\n",
       " 'eating',\n",
       " 'against',\n",
       " 'shows',\n",
       " 'sign',\n",
       " 'anymore',\n",
       " 'star',\n",
       " 'forgot',\n",
       " 'vote',\n",
       " 'ice',\n",
       " 'pass',\n",
       " 'act',\n",
       " 'along',\n",
       " 'lil',\n",
       " 'area',\n",
       " 'link',\n",
       " 'body',\n",
       " 'parents',\n",
       " 'spend',\n",
       " 'alone',\n",
       " 'available',\n",
       " 'bus',\n",
       " 'dc',\n",
       " 'ideas',\n",
       " 'town',\n",
       " 'public',\n",
       " 'pic',\n",
       " 'met',\n",
       " 'important',\n",
       " 'easy',\n",
       " 'stand',\n",
       " 'voice',\n",
       " 'dreams',\n",
       " 'ride',\n",
       " 'sister',\n",
       " 'feels',\n",
       " 'april',\n",
       " 'fight',\n",
       " 'opening',\n",
       " 'songs',\n",
       " 're',\n",
       " 'thoughts',\n",
       " 'store',\n",
       " 'movies',\n",
       " 'under',\n",
       " 'fb',\n",
       " 'baseball',\n",
       " 'center',\n",
       " 'strong',\n",
       " 'couple',\n",
       " 'sucks',\n",
       " 'weve',\n",
       " 'retweet',\n",
       " 'nigga',\n",
       " 'heading',\n",
       " 'national',\n",
       " 'sitting',\n",
       " 'test',\n",
       " 'hurt',\n",
       " 'moving',\n",
       " 'missing',\n",
       " 'months',\n",
       " 'catch',\n",
       " 'hold',\n",
       " 'lucky',\n",
       " 'somebody',\n",
       " 'blue',\n",
       " 'sharing',\n",
       " 'concert',\n",
       " 'community',\n",
       " 'arent',\n",
       " 'enjoyed',\n",
       " '2nd',\n",
       " 'march',\n",
       " 'program',\n",
       " 'wear',\n",
       " 'studio',\n",
       " 'quite',\n",
       " 'holiday',\n",
       " 'seattle',\n",
       " 'truth',\n",
       " 'cover',\n",
       " 'played',\n",
       " 'hang',\n",
       " 'short',\n",
       " 'enjoying',\n",
       " 'mention',\n",
       " 'kill',\n",
       " 'door',\n",
       " 'tuesday',\n",
       " 'questions',\n",
       " 'absolutely',\n",
       " 'boston',\n",
       " 'wit',\n",
       " 'tip',\n",
       " 'loves',\n",
       " 'straight',\n",
       " 'yours',\n",
       " 'cat',\n",
       " 'sun',\n",
       " 'company',\n",
       " 'jesus',\n",
       " 'library',\n",
       " 'vegas',\n",
       " 'congratulations',\n",
       " 'across',\n",
       " 'gym',\n",
       " 'extra',\n",
       " 'chicken',\n",
       " 'trust',\n",
       " 'smh',\n",
       " 'putting',\n",
       " 'card',\n",
       " 'finish',\n",
       " 'rip',\n",
       " 'lead',\n",
       " 'often',\n",
       " 'release',\n",
       " 'shoot',\n",
       " 'm',\n",
       " 'contact',\n",
       " 'die',\n",
       " 'mt',\n",
       " 'kinda',\n",
       " 'details',\n",
       " 'green',\n",
       " 'pizza',\n",
       " 'needed',\n",
       " 'nobody',\n",
       " 'afternoon',\n",
       " 'obama',\n",
       " 'type',\n",
       " 'paper',\n",
       " 'message',\n",
       " 'yep',\n",
       " 'chris',\n",
       " 'air',\n",
       " 'happened',\n",
       " 'xd',\n",
       " 'due',\n",
       " 'record',\n",
       " 'shut',\n",
       " 'beach',\n",
       " 'watched',\n",
       " 'eye',\n",
       " 'telling',\n",
       " 'chocolate',\n",
       " 'folks',\n",
       " 'celebrate',\n",
       " 'hoping',\n",
       " 'case',\n",
       " 'exactly',\n",
       " '♥',\n",
       " 'wife',\n",
       " 'sit',\n",
       " 'nap',\n",
       " 'breakfast',\n",
       " 'interview',\n",
       " 'suck',\n",
       " 'btw',\n",
       " 'goal',\n",
       " 'fat',\n",
       " 'child',\n",
       " 'especially',\n",
       " 'wtf',\n",
       " 'step',\n",
       " '50',\n",
       " 'sex',\n",
       " 'laugh',\n",
       " 'calling',\n",
       " 'road',\n",
       " 'cheese',\n",
       " 'design',\n",
       " 'photo',\n",
       " 'sir',\n",
       " 'boyfriend',\n",
       " 'sports',\n",
       " 'minute',\n",
       " 'roll',\n",
       " 'training',\n",
       " 'data',\n",
       " 'wednesday',\n",
       " 'de',\n",
       " 'advice',\n",
       " 'problems',\n",
       " 'near',\n",
       " 'mother',\n",
       " 'possible',\n",
       " 'serious',\n",
       " 'jealous',\n",
       " 'fantastic',\n",
       " 'aw',\n",
       " 'shopping',\n",
       " 'evening',\n",
       " 'church',\n",
       " 'asking',\n",
       " 'buddy',\n",
       " 'attention',\n",
       " 'hungry',\n",
       " 'death',\n",
       " 'happens',\n",
       " 'computer',\n",
       " 'sell',\n",
       " 'bet',\n",
       " 'ticket',\n",
       " 'looked',\n",
       " 'issue',\n",
       " 'driving',\n",
       " 'age',\n",
       " 'box',\n",
       " 'wedding',\n",
       " 'plans',\n",
       " 'etc',\n",
       " 'law',\n",
       " 'completely',\n",
       " 'lie',\n",
       " 'study',\n",
       " 'market',\n",
       " 'sale',\n",
       " '2011',\n",
       " 'track',\n",
       " 'complete',\n",
       " 'apparently',\n",
       " 'pants',\n",
       " 'health',\n",
       " 'realize',\n",
       " 'personal',\n",
       " 'goodnight',\n",
       " 'lord',\n",
       " 'winning',\n",
       " 'takes',\n",
       " 'speak',\n",
       " 'joke',\n",
       " 'lovely',\n",
       " 'plus',\n",
       " 'min',\n",
       " 'quick',\n",
       " 'energy',\n",
       " 'ladies',\n",
       " 'brand',\n",
       " 'officially',\n",
       " 'internet',\n",
       " 'um',\n",
       " 'mobile',\n",
       " 'artist',\n",
       " 'practice',\n",
       " 'content',\n",
       " 'spot',\n",
       " 'piece',\n",
       " 'shop',\n",
       " 'color',\n",
       " 'bought',\n",
       " 'stuck',\n",
       " 'tuned',\n",
       " 'whatever',\n",
       " 'june',\n",
       " 'longer',\n",
       " 'stage',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list((pd.Series(sentences).value_counts() > 10).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def get_words_with_nplus_frequency(data, n):\n",
    "    \"\"\"\n",
    "    Get words appearing n or more times in the data.\n",
    "    \"\"\"\n",
    "    out = list((pd.Series(data).value_counts() >= n).index)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "پیش‌پردازش داده‌ها\n",
    "</font>\n",
    "</h2>\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در این مرحله داده‌ها برای استفاده در مدل آماده می‌شوند. این کار شامل تقسیم داده‌ها به داده‌های آموزشی و آزمایشی و سپس جایگزینی کلمات خارج از واژه‌نامه با <b>&lt;unk&gt;</b> است. همچنین برای واژه‌نامه، کلمات با فرکانس n یا بیشتر استخراج می‌شوند.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def preprocess_data(train_data, count_threshold):\n",
    "    \"\"\"\n",
    "    Preprocess data to handle out-of-vocabulary words by replacing them with <unk>.\n",
    "    \"\"\"\n",
    "    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)\n",
    "    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary)\n",
    "\n",
    "    return train_data_replaced, vocabulary\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "train_sentences = sentences[:int(0.8 * len(sentences))]\n",
    "test_sentences = sentences[int(0.8 * len(sentences)):]\n",
    "train_sentences_replaced, vocabulary = preprocess_data(train_sentences, count_threshold=5)\n",
    "test_sentences_replaced = replace_oov_words_by_unk(test_sentences, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "ایجاد n-gramها\n",
    "</font>\n",
    "</h2>\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در این مرحله باید تابعی بنویسید که n-gramها را از مجموعه توکن‌ها تولید کند. یک n-gram مجموعه‌ای از n کلمه است که در کنار هم آمده‌اند. این کار به مدل کمک می‌کند تا الگوهای موجود در داده‌ها را یاد بگیرد.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no', 'lmao', 'she', 'used']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences_replaced[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def generate_ngrams(tokens, n):\n",
    "    \"\"\"\n",
    "    Generate n-grams from a list of tokens.\n",
    "    \"\"\"\n",
    "    b = []\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        a = tokens[i:i+n]\n",
    "        b.append(a)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [teachers, no, lmao]\n",
       "1                   [no, lmao, she]\n",
       "2                 [lmao, she, used]\n",
       "3                   [she, used, to]\n",
       "4                   [used, to, sit]\n",
       "                    ...            \n",
       "118327    [sent, ticket, yesterday]\n",
       "118328    [ticket, yesterday, thnx]\n",
       "118329       [yesterday, thnx, can]\n",
       "118330            [thnx, can, take]\n",
       "118331             [can, take, you]\n",
       "Length: 118332, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm = generate_ngrams(test_sentences_replaced, 3)\n",
    "pd.Series(mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "ساخت مدل n-gram\n",
    "</font>\n",
    "</h2>\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در این مرحله شما باید مدل n-gram را بسازید. مدل باید از جملات توکنایز شده استفاده کرده و تعداد وقوع n-gramها را محاسبه کند. این مدل برای تولید جملات جدید و محاسبه احتمال وقوع جملات استفاده می‌شود.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('how', 'are', 'you'): 1,\n",
       "         ('are', 'you', 'btw'): 1,\n",
       "         ('you', 'btw', 'thanks'): 1,\n",
       "         ('btw', 'thanks', 'for'): 1,\n",
       "         ('thanks', 'for', 'the'): 1,\n",
       "         ('for', 'the', 'rt'): 1,\n",
       "         ('the', 'rt', 'you'): 1,\n",
       "         ('rt', 'you', 'gon'): 1,\n",
       "         ('you', 'gon', 'na'): 1,\n",
       "         ('gon', 'na', 'be'): 1,\n",
       "         ('na', 'be', 'in'): 1,\n",
       "         ('be', 'in', 'dc'): 1,\n",
       "         ('in', 'dc', 'anytime'): 1,\n",
       "         ('dc', 'anytime', 'soon'): 1,\n",
       "         ('anytime', 'soon', 'love'): 1,\n",
       "         ('soon', 'love', 'to'): 1,\n",
       "         ('love', 'to', 'see'): 1,\n",
       "         ('to', 'see', 'you'): 1,\n",
       "         ('see', 'you', 'been'): 1,\n",
       "         ('you', 'been', 'way'): 1,\n",
       "         ('been', 'way', 'way'): 1,\n",
       "         ('way', 'way', 'too'): 1,\n",
       "         ('way', 'too', 'long'): 1,\n",
       "         ('too', 'long', 'when'): 1,\n",
       "         ('long', 'when', 'you'): 1,\n",
       "         ('when', 'you', 'meet'): 1,\n",
       "         ('you', 'meet', 'someone'): 1,\n",
       "         ('meet', 'someone', 'special'): 1,\n",
       "         ('someone', 'special', 'youll'): 1,\n",
       "         ('special', 'youll', 'know'): 1,\n",
       "         ('youll', 'know', 'your'): 1,\n",
       "         ('know', 'your', 'heart'): 1,\n",
       "         ('your', 'heart', 'will'): 1,\n",
       "         ('heart', 'will', 'beat'): 1,\n",
       "         ('will', 'beat', 'more'): 1,\n",
       "         ('beat', 'more', 'rapidly'): 1,\n",
       "         ('more', 'rapidly', 'and'): 1,\n",
       "         ('rapidly', 'and', 'youll'): 1,\n",
       "         ('and', 'youll', 'smile'): 1,\n",
       "         ('youll', 'smile', 'for'): 1,\n",
       "         ('smile', 'for', 'no'): 1,\n",
       "         ('for', 'no', 'reason'): 1,\n",
       "         ('no', 'reason', 'theyve'): 1,\n",
       "         ('reason', 'theyve', 'decided'): 1,\n",
       "         ('theyve', 'decided', 'its'): 1,\n",
       "         ('decided', 'its', 'more'): 1,\n",
       "         ('its', 'more', 'fun'): 1,\n",
       "         ('more', 'fun', 'if'): 1,\n",
       "         ('fun', 'if', 'i'): 1,\n",
       "         ('if', 'i', 'dont'): 1,\n",
       "         ('i', 'dont', 'so'): 1,\n",
       "         ('dont', 'so', 'tired'): 1,\n",
       "         ('so', 'tired', 'd'): 1,\n",
       "         ('tired', 'd', 'played'): 1,\n",
       "         ('d', 'played', 'lazer'): 1,\n",
       "         ('played', 'lazer', 'tag'): 1,\n",
       "         ('lazer', 'tag', 'ran'): 1,\n",
       "         ('tag', 'ran', 'a'): 1,\n",
       "         ('ran', 'a', 'lot'): 1,\n",
       "         ('a', 'lot', 'd'): 1,\n",
       "         ('lot', 'd', 'ughh'): 1,\n",
       "         ('d', 'ughh', 'going'): 1,\n",
       "         ('ughh', 'going', 'to'): 1,\n",
       "         ('going', 'to', 'sleep'): 1,\n",
       "         ('to', 'sleep', 'like'): 1,\n",
       "         ('sleep', 'like', 'in'): 1,\n",
       "         ('like', 'in', '5'): 1,\n",
       "         ('in', '5', 'minutes'): 1,\n",
       "         ('5', 'minutes', 'words'): 1,\n",
       "         ('minutes', 'words', 'from'): 1,\n",
       "         ('words', 'from', 'a'): 1,\n",
       "         ('from', 'a', 'complete'): 1,\n",
       "         ('a', 'complete', 'stranger'): 1,\n",
       "         ('complete', 'stranger', 'made'): 1,\n",
       "         ('stranger', 'made', 'my'): 1,\n",
       "         ('made', 'my', 'birthday'): 1,\n",
       "         ('my', 'birthday', 'even'): 1,\n",
       "         ('birthday', 'even', 'better'): 1,\n",
       "         ('even', 'better', 'first'): 1,\n",
       "         ('better', 'first', 'cubs'): 1,\n",
       "         ('first', 'cubs', 'game'): 1,\n",
       "         ('cubs', 'game', 'ever'): 1,\n",
       "         ('game', 'ever', 'wrigley'): 1,\n",
       "         ('ever', 'wrigley', 'field'): 1,\n",
       "         ('wrigley', 'field', 'is'): 1,\n",
       "         ('field', 'is', 'gorgeous'): 1,\n",
       "         ('is', 'gorgeous', 'this'): 1,\n",
       "         ('gorgeous', 'this', 'is'): 1,\n",
       "         ('this', 'is', 'perfect'): 1,\n",
       "         ('is', 'perfect', 'go'): 1,\n",
       "         ('perfect', 'go', 'cubs'): 1,\n",
       "         ('go', 'cubs', 'go'): 1,\n",
       "         ('cubs', 'go', 'i'): 1,\n",
       "         ('go', 'i', 'no'): 1,\n",
       "         ('i', 'no', 'i'): 1,\n",
       "         ('no', 'i', 'get'): 1,\n",
       "         ('i', 'get', 'another'): 1,\n",
       "         ('get', 'another', 'day'): 1,\n",
       "         ('another', 'day', 'off'): 1,\n",
       "         ('day', 'off', 'from'): 1})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "a = generate_ngrams(train_sentences_replaced, 3)\n",
    "a = a[:100]\n",
    "Counter(tuple(a) for a in a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N-gram Model (Sample):\n",
      "('how', 'are', 'you'): 53\n",
      "('are', 'you', 'btw'): 1\n",
      "('you', 'btw', 'thanks'): 1\n",
      "('btw', 'thanks', 'for'): 2\n",
      "('thanks', 'for', 'the'): 376\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_ngram_model(sentences, n):\n",
    "    \"\"\"\n",
    "    Build an n-gram model from the tokenized sentences.\n",
    "    Returns a dictionary with n-grams as keys and their counts as values.\n",
    "    \"\"\"\n",
    "    # Generate n-grams\n",
    "    ngrams = generate_ngrams(sentences, n)\n",
    "    # Count n-grams using Counter\n",
    "    ngram_counts = Counter(tuple(ngram) for ngram in ngrams)\n",
    "    return dict(ngram_counts)\n",
    "\n",
    "# Build the n-gram model\n",
    "n = 3\n",
    "ngram_model = build_ngram_model(train_sentences_replaced, n)\n",
    "\n",
    "# Display a sample of the n-gram model\n",
    "print(\"\\nN-gram Model (Sample):\")\n",
    "for key, value in list(ngram_model.items())[:5]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_keys' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mngram_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'dict_keys' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "تولید جملات\n",
    "</font>\n",
    "</h2>\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در این بخش شما باید تابعی بنویسید که با استفاده از مدل n-gram جملات جدیدی تولید کند. این تابع باید به طور تصادفی کلمات را از مدل انتخاب کرده و جمله‌ای را بسازد. طول جمله‌ها نیز محدودیت دارد.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Random Sentence:\n",
      "the roboto boardi dont have the memories ill never write a hit hi naddem this\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "def generate_sentence(ngram_model, n, sentence_length=15):\n",
    "    \"\"\"\n",
    "    Generate a random sentence of specified length using an n-gram model.\n",
    "    \n",
    "    Args:\n",
    "        ngram_model (dict): Dictionary with n-grams (tuples) as keys and counts as values.\n",
    "        n (int): The n-gram order (e.g., 3 for trigrams).\n",
    "        sentence_length (int): Number of words in the generated sentence.\n",
    "    \n",
    "    Returns:\n",
    "        str: A random sentence with specified length.\n",
    "    \"\"\"\n",
    "    # Convert n-grams to list for weighted sampling\n",
    "    ngrams = list(ngram_model.keys())\n",
    "    weights = list(ngram_model.values())\n",
    "    \n",
    "    # Start with a random n-gram\n",
    "    current_ngram = random.choices(ngrams, weights=weights, k=1)[0]\n",
    "    sentence = list(current_ngram)\n",
    "    \n",
    "    # Generate subsequent words\n",
    "    while len(sentence) < sentence_length:\n",
    "        # Find n-grams that start with the last (n-1) words of current sentence\n",
    "        prefix = tuple(sentence[-(n-1):]) if n > 1 else ()\n",
    "        candidates = [ng for ng in ngrams if ng[:n-1] == prefix]\n",
    "        candidate_weights = [ngram_model[ng] for ng in candidates]\n",
    "        \n",
    "        if not candidates:\n",
    "            # If no matching n-grams, pick a random n-gram to continue\n",
    "            current_ngram = random.choices(ngrams, weights=weights, k=1)[0]\n",
    "            sentence.extend(current_ngram)\n",
    "        else:\n",
    "            # Sample next n-gram based on counts\n",
    "            next_ngram = random.choices(candidates, weights=candidate_weights, k=1)[0]\n",
    "            # Append only the last word of the next n-gram\n",
    "            sentence.append(next_ngram[-1])\n",
    "    \n",
    "    # Trim to exact length if necessary\n",
    "    sentence = sentence[:sentence_length]\n",
    "    \n",
    "    # Join words into a sentence\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "# Example usage\n",
    "n = 3\n",
    "random_sentence = generate_sentence(ngram_model, n, sentence_length=15)\n",
    "print(\"\\nGenerated Random Sentence:\")\n",
    "print(random_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "محاسبه پرپلکسیتی\n",
    "</font>\n",
    "</h2>\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در این مرحله، شما باید پرپلکسیتی یک جمله را محاسبه کنید. پرپلکسیتی معیاری است که نشان می‌دهد مدل چقدر خوب است. هر چه پرپلکسیتی کمتر باشد، مدل بهتر است. شما باید این تابع را با استفاده از مدل n-gram و با اعمال هم‌سطحی (smoothing) بنویسید.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def calculate_perplexity(sentence, ngram_model, n, smoothing_factor=1e-10):\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a sentence using the n-gram model with smoothing.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): The input sentence.\n",
    "        ngram_model (dict): Dictionary with n-grams (tuples) as keys and counts as values.\n",
    "        n (int): The n-gram order.\n",
    "        smoothing_factor (float): Smoothing factor for unseen n-grams.\n",
    "    \n",
    "    Returns:\n",
    "        float: Perplexity of the sentence.\n",
    "    \"\"\"\n",
    "    # Tokenize sentence\n",
    "    tokens = sentence.split()\n",
    "    if len(tokens) < n:\n",
    "        return float('inf')  # Return infinity for sentences too short for n-grams\n",
    "    \n",
    "    # Vocabulary size (approximated as total unique n-grams)\n",
    "    vocab_size = len(set(ngram_model.keys()))\n",
    "    \n",
    "    log_prob = 0.0\n",
    "    total_ngrams = sum(ngram_model.values())  # Total n-gram counts\n",
    "    \n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        # Extract n-gram\n",
    "        ngram = tuple(tokens[i:i+n])\n",
    "        prefix = ngram[:-1] if n > 1 else ()  # Prefix for conditional probability\n",
    "        \n",
    "        # Count prefix occurrences\n",
    "        prefix_count = sum(ngram_model.get((prefix + (w,)), 0) for w in set(tokens))\n",
    "        \n",
    "        # Apply smoothing\n",
    "        ngram_count = ngram_model.get(ngram, 0) + smoothing_factor\n",
    "        prefix_count = prefix_count + smoothing_factor * vocab_size if prefix_count > 0 else smoothing_factor * vocab_size\n",
    "        \n",
    "        # Calculate probability with smoothing\n",
    "        prob = ngram_count / prefix_count\n",
    "        log_prob += math.log2(prob) if prob > 0 else float('-inf')\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    if log_prob == float('-inf'):\n",
    "        return float('inf')\n",
    "    avg_log_prob = -log_prob / (len(tokens) - n + 1)\n",
    "    return 2 ** avg_log_prob\n",
    "\n",
    "# Calculate perplexity for generated sentences\n",
    "perplexities = [calculate_perplexity(sentence, ngram_model, n) for sentence in random_sentence]\n",
    "\n",
    "# Plot the perplexity distribution\n",
    "#plt.hist(perplexities, bins=20, edgecolor='black')\n",
    "#plt.title('Perplexity Distribution of Generated Sentences')\n",
    "#plt.xlabel('Perplexity')\n",
    "#plt.ylabel('Frequency')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "ذخیره نتایج و فشرده‌سازی فایل‌ها\n",
    "</font>\n",
    "</h2>\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در این بخش، جملات تولید شده و پرپلکسیتی های آن‌ها در یک فایل CSV ذخیره می‌شوند و سپس این فایل‌ها به همراه نوت‌بوک <b>auto_complete.ipynb</b> در یک فایل فشرده ZIP ذخیره می‌شوند تا برای ارزیابی و ارسال آماده شوند.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "500 sentences with perplexities saved to 'generated_sentences.csv'\n"
     ]
    }
   ],
   "source": [
    "generated_sentences = [generate_sentence(ngram_model, n) for _ in range(500)]\n",
    "perplexities = [calculate_perplexity(sentence, ngram_model, n) for sentence in generated_sentences]\n",
    "\n",
    "results = pd.DataFrame({'generated_sentence': generated_sentences, 'perplexity': perplexities})\n",
    "results.to_csv(\"generated_sentences.csv\", index=False)\n",
    "\n",
    "print(\"\\n500 sentences with perplexities saved to 'generated_sentences.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added auto_complete.ipynb to submission.zip\n",
      "Added generated_sentences.csv to submission.zip\n",
      "Files have been zipped into submission.zip, you can now submit it!\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# List of files to be zipped\n",
    "files_to_zip = ['auto_complete.ipynb', 'generated_sentences.csv']\n",
    "zip_filename = 'submission.zip'\n",
    "\n",
    "# Create the zip file\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    for file in files_to_zip:\n",
    "        try:\n",
    "            zipf.write(file)\n",
    "            print(f\"Added {file} to {zip_filename}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: {file} not found. Skipping.\")\n",
    "\n",
    "print(f\"Files have been zipped into {zip_filename}, you can now submit it!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
